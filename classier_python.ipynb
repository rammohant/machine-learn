{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Environment Set-Up\n"
      ],
      "metadata": {
        "id": "6pshS--DtbvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare Google Drive Mount + Modules"
      ],
      "metadata": {
        "id": "PYz5WLf-z86N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you get \"Transport not found\" error: \n",
        "# STEP 1: Reset runtime (i.e. terminate session)\n",
        "# STEP 2: Run the following code \n",
        "!fusermount -u drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIGd5RK5Fkvb",
        "outputId": "fce6e3cf-d2cf-4ee2-fe96-f51fccb6a902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fusermount: failed to unmount /content/drive: No such file or directory\n",
            "/bin/bash: google-drive-ocamlfuse: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "my_drive_path = '/content/drive/MyDrive/'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import preprocessing, datasets    \n",
        "from sklearn import metrics\n",
        "import math"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6b0K9DqozyE",
        "outputId": "c6acbb8e-01ce-47e7-812d-c116d1ec2321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation \n"
      ],
      "metadata": {
        "id": "1uXYFqIXulhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create Pfeature and 2-gram features"
      ],
      "metadata": {
        "id": "Mni9So2bIE0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(file, testing_bool):\n",
        "  # df = pd.read_csv(shared_folder_path + 'sequences_training.txt', header=None)\n",
        "  df = pd.read_csv('/content/drive/MyDrive/rams/fall 2022/CMSC 435/CMSC 435 Project/' + file, header=None)\n",
        "  if (testing_bool == 0):\n",
        "    df.columns = ['sequence', 'label']\n",
        "  elif (testing_bool == 1): \n",
        "    df.columns = ['sequence']\n",
        "  return df\n",
        "\n",
        "# FEATURE: Pfeature -----------------------------------------------------------\n",
        "def prep_for_pfeature(df, testing_bool):\n",
        "  # python setup.py install --user\n",
        "  if (testing_bool == 0): \n",
        "    df = df.drop(columns=['label'])\n",
        "  print(\"INPUT DF SIZE: \", df.shape)\n",
        "  df.to_csv(r'protein2.seq', header=None, index=None, sep='\\t', mode='a')\n",
        "\n",
        "  # https://wiki.thegpm.org/wiki/Amino_acid_symbols\n",
        "  fh = open('protein2.seq','r').read()\n",
        "  fh = fh.replace('Z', 'E')  # Glutamic acid (E) or Glutamine (Q)\n",
        "  fh = fh.replace('X', '')  # Unknown amino acid\n",
        "  fh = fh.replace('U', '')  # Unusual translation\n",
        "  # unique_chars = set(fh)\n",
        "  # print(\"LEN\", sorted(unique_chars))\n",
        "\n",
        "  text_file = open(\"protein2.seq\", \"w\")\n",
        "  n = text_file.write(fh)\n",
        "  text_file.close()\n",
        "\n",
        "#HERE SHAHAD\n",
        "def create_pfeature(read_file, input_file, output_file, testing_bool): \n",
        "\n",
        "  # Remove previous files\n",
        "  import os\n",
        "  if os.path.exists(input_file):\n",
        "    os.remove(input_file)\n",
        "\n",
        "  if os.path.exists(output_file):\n",
        "    os.remove(output_file)\n",
        "\n",
        "  df2 = read_data(read_file, testing_bool)\n",
        "  %cd \"/content/drive/MyDrive/rams/fall 2022/CMSC 435/CMSC 435 Project/pfeature_standalone\"\n",
        "  prep_for_pfeature(df2, testing_bool)\n",
        "\n",
        "  import subprocess\n",
        "  os.system(\"python pfeature_comp.py -i \" + input_file + \" -o \" + output_file + \" -j CTC\")\n",
        "  # !python pfeature_comp.py -i protein2.seq -o comp.txt -j CTC\n",
        "  comp_df = pd.read_csv(\"comp.txt\")\n",
        "  print(\"COMP DF SIZE\", comp_df.shape)\n",
        "\n",
        "  # os.system(\"python pfeature_bin.py -i \" + input_file + \" -o \" + output_file + \" -j CTC\")\n",
        "  # bin_df = pd.read_csv(\"bin.txt\")\n",
        "  # print(bin_df)\n",
        "  # df2 = pd.concat([df2, comp_df, bin_df], axis=1)\n",
        "  df2 = pd.concat([df2, comp_df], axis=1)\n",
        "\n",
        "  print(\"FINAL DF SIZE\", df2.shape)\n",
        "\n",
        "  return df2\n",
        "\n",
        "# FEATURE: 2 gram -----------------------------------------------------------\n",
        "def two_or_three_gram(df_row, num_of_gram):\n",
        "  arr = np.array(['V', 'Y', 'T', 'G', 'D', 'C', 'K', 'S', 'H', 'E', 'F', 'Q', 'R', 'A', 'N', 'P', 'I', 'W', 'L', 'M'])\n",
        "  # storing the pairs at the appropriate key, will take the length of the value (array) of each key to get the count\n",
        "  if num_of_gram == 2:\n",
        "    list_of_potential_pairs = []\n",
        "    # print(len(arr), len(df_row))\n",
        "    for i in range(len(arr)):\n",
        "      for j in range(len(arr)):\n",
        "        list_of_potential_pairs.append([arr[i], arr[j]])\n",
        "\n",
        "    # storing the potential pairs as keys in a dict, storing the values as 0 for now\n",
        "    dict_of_pairs = {}\n",
        "    for pairs in list_of_potential_pairs:\n",
        "      dict_of_pairs[f\"{pairs}\"] = []   \n",
        "\n",
        "    for i in range(len(df_row)-1):\n",
        "        pair = f\"{[df_row[i], df_row[i+1]]}\"\n",
        "        if pair in dict_of_pairs:\n",
        "          dict_of_pairs[pair].append(pair)\n",
        "  \n",
        "  if num_of_gram == 3: \n",
        "    list_of_potential_pairs = []\n",
        "    for i in range(len(arr)):\n",
        "      for j in range(len(arr)):\n",
        "        for k in range(len(arr)):\n",
        "          list_of_potential_pairs.append([arr[i], arr[j], arr[k]])\n",
        "\n",
        "    # storing the potential pairs as keys in a dict, storing the values as 0 for now\n",
        "    dict_of_pairs = {}\n",
        "    for pairs in list_of_potential_pairs:\n",
        "      dict_of_pairs[f\"{pairs}\"] = []   \n",
        "\n",
        "    for i in range(len(df_row)-2):\n",
        "      pair = f\"{[df_row[i], df_row[i+1], df_row[i+2]]}\"\n",
        "      if pair in dict_of_pairs:\n",
        "          dict_of_pairs[pair].append(pair)\n",
        "\n",
        "  for key, value in dict_of_pairs.items():\n",
        "    count = len(value)\n",
        "    dict_of_pairs[key] = count\n",
        "\n",
        "  return list(dict_of_pairs.values())\n",
        "\n",
        "# FEATURE: Creates unique_val, unique_val_count, aa_count_freq -----------------------------------------------------------\n",
        "def create_count_features(df):\n",
        "\n",
        "  sequence_length = df['sequence'].str.len() # amino acid sequence length\n",
        "  df = df.assign(length = sequence_length)\n",
        "\n",
        "  arr = df['sequence'].to_list()\n",
        "  unique_values = []\n",
        "  count = []\n",
        "  aa_count_freq = []\n",
        "  all_aa = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
        "\n",
        "  for a in arr:\n",
        "\n",
        "      # Get unique amino acids\n",
        "      unique_val_set = list(set(a))\n",
        "      unique_values.append(sorted(unique_val_set))\n",
        "      \n",
        "      # Count of unique amino acids in each sequence\n",
        "      count.append(len(unique_val_set))\n",
        "\n",
        "      aa_count_totals = []\n",
        "\n",
        "      for aa in all_aa:\n",
        "       aa_count_totals.append(round(a.count(aa)/len(a),2))\n",
        "      aa_count_freq.append(aa_count_totals)\n",
        "\n",
        "  df = df.assign(unique_val = unique_values)\n",
        "  df = df.assign(unique_val_count = count)\n",
        "  df = df.assign(aa_count_freq = aa_count_freq)\n",
        "\n",
        "  return df\n",
        "\n",
        "# FEATURE: Create encoder object and fit array -----------------------------------------------------------\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Parse through and encode each individual sequence \n",
        "def ordinal_encoder(my_array):\n",
        "   label_encoder.fit(my_array)\n",
        "   integer_encoded = label_encoder.transform(my_array)\n",
        "   float_encoded = integer_encoded.astype(float)\n",
        "   j = 0\n",
        "   for i in range(0,20):\n",
        "      j += .05\n",
        "      float_encoded[float_encoded == i] = round(j, 2)\n",
        "   return float_encoded\n",
        "\n",
        "# FEATURE: Convert amino acid genome df column into encoded array\n",
        "def create_encoded_genome_col(df):\n",
        "\n",
        "  # Convert amino acid genome column into array\n",
        "  arr = df['sequence'].to_list()\n",
        "\n",
        "  feature_vectors = []\n",
        "  for a in arr:\n",
        "      # Get unique amino acids and count of unique amino acids in each sequence\n",
        "      unique_values = set(a)\n",
        "      count = len(unique_values)\n",
        "\n",
        "      # Encode sequence\n",
        "      encoding = ordinal_encoder(list(a))\n",
        "      feature_vectors.append(encoding)\n",
        "\n",
        "  # Create new column of encoded sequences\n",
        "  df = df.assign(encoded = feature_vectors)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "gnyB89yUrxDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = create_pfeature(\"sequences_training.txt\", \"protein2.seq\", \"comp.txt\", 0)\n",
        "df2 = create_count_features(df2)\n",
        "df2 = create_encoded_genome_col(df2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhhfDV7cgX2A",
        "outputId": "dc7b45c6-88a4-4970-e2be-6ab77675af25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1fuA8qot-MwgQ6H4rA5nuxA3l8Za7jqqS/rams/fall 2022/CMSC 435/CMSC 435 Project/pfeature_standalone\n",
            "INPUT DF SIZE:  (8795, 1)\n",
            "COMP DF SIZE (26385, 343)\n",
            "FINAL DF SIZE (26385, 345)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding 2 gram features to the dataset -------------------------------------------------\n",
        "\n",
        "# assigning two gram features to the dataset, but grouped\n",
        "df2 = df2.astype(object)\n",
        "df2.assign(two_gram_features = np.zeros([len(df2),400]))\n",
        "\n",
        "for i, row in enumerate(df2.sequence.to_numpy()):\n",
        "  whole_pair_list = two_or_three_gram(row,2) \n",
        "  divide_pair_list_into_groups = np.array(whole_pair_list).reshape(20,20)\n",
        "  list_of_counts = []\n",
        "  for group in divide_pair_list_into_groups:\n",
        "    # trying to find better ways to group these\n",
        "    # 1 way: take the sum of one group and make it that value, but scale it to be between 0 and 1\n",
        "    sum = np.sum(group) \n",
        "    sum = round(sum * .01, 2)\n",
        "    list_of_counts.append(sum) \n",
        "\n",
        "  # 2nd way: take the max value of that group, and add the index to it (index of the max has importance to it, adding is one way to show it)\n",
        "  #   group = np.array(group)\n",
        "  #   max = np.max(group)\n",
        "  #   index = group.argmax() \n",
        "  #   max_plus_index = round(max * .01, 2) + index\n",
        "  #   list_of_counts.append(max_plus_index) \n",
        "  df2.at[i,\"two_gram_features\"] = list_of_counts"
      ],
      "metadata": {
        "id": "ULDBnZnnHueq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "0138175b-b653-4b45-9a35-3db2070e16c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-51d1ae42049c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mwhole_pair_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwo_or_three_gram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mdivide_pair_list_into_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhole_pair_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mlist_of_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-463a0c92d78b>\u001b[0m in \u001b[0;36mtwo_or_three_gram\u001b[0;34m(df_row, num_of_gram)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0mdict_of_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{pairs}\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mpair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{[df_row[i], df_row[i+1]]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdict_of_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DF COLUMNS: \", df2.columns)\n",
        "print(\"DF SHAPE: \", df2.shape)"
      ],
      "metadata": {
        "id": "RqiBQQHVKm1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9q-iciUXKnCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Implementation"
      ],
      "metadata": {
        "id": "wjjRnthNucKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create feature vector + training dataset"
      ],
      "metadata": {
        "id": "GzAWUu3JzKxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib.logger import print_function\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "\n",
        "from sklearn.utils.validation import check_array\n",
        "\n",
        "def prep_for_model(df2, testing_bool): \n",
        "  y = 0\n",
        "\n",
        "  print(\"FINAL COLUMNS PRE\", df2.columns)\n",
        "\n",
        "  # Drop features we're not looking at  \n",
        "  if (testing_bool == 0):\n",
        "    df_final = df2.drop(columns=['label', 'sequence','unique_val', 'unique_val_count', 'encoded'])       \n",
        "    labels = df2['label'].to_numpy()\n",
        "    y = labels\n",
        "  elif (testing_bool == 1): \n",
        "    df_final = df2.drop(columns=['sequence','unique_val', 'unique_val_count', 'encoded'])\n",
        "\n",
        "  print(\"FINAL COLUMNS POST\", df_final.columns)\n",
        "  print(\"AA COLUMNS\", len(df_final.aa_count_freq[0]))\n",
        "\n",
        "  # Combining all the features we want to use and adding it to new feature vector column\n",
        "  features = df_final.to_numpy()\n",
        "  feature_vector = list()\n",
        "  for i in range(len(features)):\n",
        "    np.set_printoptions(suppress=True)\n",
        "    flattened_features = np.hstack(np.array(features[i]))\n",
        "    feature_vector.append(flattened_features)\n",
        "  df_final['feature_vector'] = feature_vector\n",
        "\n",
        "  # features and lables will be used to run the model\n",
        "  features = df_final['feature_vector'].to_numpy()\n",
        "\n",
        "  # Numpy is weird, few workarounds to unpack this 2d array correctly\n",
        "  X = np.asarray(features).reshape(-1,1)\n",
        "  X = np.array([x[0] for x in X])\n",
        "\n",
        "  # Resampling data (only the classes with the least data)\n",
        "  sm = SMOTE(random_state=42, sampling_strategy={\"DRNA\":200, \"RNA\": 750, \"DNA\":550})\n",
        "\n",
        "  # Feature Scaling for input features (if we want to scale features further down the line)\n",
        "  # scaler = preprocessing.StandardScaler()\n",
        "  # x_scaled = scaler.fit_transform(x)\n",
        "\n",
        "  # Create training CSV file (501 columns appraoch)\n",
        "  columns = ['sequence_length']\n",
        "  feature_vector_length = len(df_final.feature_vector[0])\n",
        "  print(\"FEATURE VECTOR LENGTH: \", feature_vector_length)\n",
        "  for i in range(0,feature_vector_length - 1):\n",
        "    temp = 'aa_' + str(i)\n",
        "    columns.append(temp)\n",
        "  # columns.append('label')\n",
        "\n",
        "  if (testing_bool == 0):\n",
        "    final_training = pd.DataFrame(list(df_final['feature_vector']), columns=columns)\n",
        "    display(final_training)\n",
        "    final_training['class'] = labels\n",
        "    # Output CSV\n",
        "    display(final_training.head(20))\n",
        "    # final_training.to_csv(\"training2.csv\")\n",
        "\n",
        "\n",
        "  return X, y, sm\n",
        "\n",
        "X, y, sm = prep_for_model(df2, 0)"
      ],
      "metadata": {
        "id": "90j4neJyzWyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross Validation + Model Implementation"
      ],
      "metadata": {
        "id": "jQ1c-DO7zFjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# classifier = MultinomialNB(alpha=0.1)\n",
        "\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# classifier = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "print(\"Number of feature before k select: \", len(X[0]))\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier()\n",
        "\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "selector = SelectFromModel(estimator=RandomForestClassifier()).fit(X, y)\n",
        "X = selector.transform(X)\n",
        "\n",
        "print(\"Number of feature after k select: \", len(X[0]))\n",
        "\n"
      ],
      "metadata": {
        "id": "lqQ60m9NLhLK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "210336b6-ae7f-4856-a270-5523284ca3ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of feature before k select:  384\n",
            "Number of feature after k select:  163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wqymax_coxIu"
      },
      "outputs": [],
      "source": [
        "# CROSS VALIDATION AND MODEL IMPLEMENTATION\n",
        "\n",
        "lst_perf_metrics = []\n",
        "final_y_prediction = list() # create empty list\n",
        "final_y_expected = list()\n",
        "\n",
        "# See more info on the output 5 fold validation data here: https://www.geeksforgeeks.org/stratified-k-fold-cross-validation/\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "\n",
        "  # Features for training and testing\n",
        "  X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
        "\n",
        "  # Class labels for training and testing\n",
        "  y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
        "\n",
        "  X_train_fold = X_train_fold.tolist()\n",
        "  y_train_fold = y_train_fold.tolist()\n",
        "  X_test_fold = X_test_fold.tolist()\n",
        "  y_test_fold = y_test_fold.tolist()\n",
        "\n",
        "  # Resampling in the cross validation\n",
        "  X_train_fold, y_train_fold = sm.fit_resample(X_train_fold, y_train_fold)\n",
        "\n",
        "  classifier.fit(X_train_fold, y_train_fold)\n",
        "  curr_y_prediction = classifier.predict(X_test_fold) \n",
        "\n",
        "  final_y_prediction.append(list(curr_y_prediction))\n",
        "  final_y_expected.append(list(y_test_fold))\n",
        "\n",
        "flattened_array_pred = [item for sublist in final_y_prediction for item in sublist]\n",
        "flattened_array_exp = [item for sublist in final_y_expected for item in sublist]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Metrics Method"
      ],
      "metadata": {
        "id": "l-tbYVlxzpia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PERFORMANCE METRICS CALCULATION\n",
        "def confusion_metrics(TP, TN, FP, FN, label_name):\n",
        "    \n",
        "    # calculate accuracy\n",
        "    conf_accuracy = 100 * (float (TP+TN)) / (float(TP + FP + TN + FN))\n",
        "    # calculate the sensitivity\n",
        "    conf_sensitivity = 100 * (TP / float(TP + FN))\n",
        "    # calculate the specificity\n",
        "    conf_specificity = 100 * (TN / float(TN + FP))\n",
        "    # calculate mcc\n",
        "    conf_mcc = float( (TP*TN) - (FP*FN) ) / math.sqrt(float((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)))\n",
        "\n",
        "    print(label_name, '-'*50)\n",
        "    print(f'Sensitivity: {round(conf_sensitivity,1)}') \n",
        "    print(f'Specificity: {round(conf_specificity,1)}') \n",
        "    print(f'Accuracy: {round(conf_accuracy,1)}') \n",
        "    #print(f'Precision: {round(conf_precision,2)}')\n",
        "    print(f'MCC: {round(conf_mcc,3)}',\"\\n\")\n",
        "\n",
        "    return conf_mcc\n",
        "\n",
        "def perf_metrics(y_prediction, y_test_fold):\n",
        "\n",
        "  # print(classification_report(y_test_fold, y_prediction), \"\\n\")\n",
        "\n",
        "  total_num_proteins = len(y_prediction)\n",
        "\n",
        "  # Get all unique sense tags\n",
        "  indices = list(set(df2['label']))\n",
        "  \n",
        "  # Using sklearn.metrics, create a confusion matrix\n",
        "  cm = metrics.confusion_matrix(y_test_fold, y_prediction, labels = indices)\n",
        "\n",
        "  cm_df = pd.DataFrame(cm,\n",
        "              index = ['A: ' + i for i in indices], \n",
        "              columns = ['P: ' + i for i in indices])\n",
        "\n",
        "  print(cm_df, '\\n')\n",
        "\n",
        "  mcc = 0\n",
        "\n",
        "  # DNA - Shahad\n",
        "  DNA_TP = cm_df[\"P: DNA\"][\"A: DNA\"]\n",
        "  DNA_TN = cm_df[\"P: nonDRNA\"][\"A: nonDRNA\"] + cm_df[\"P: DRNA\"][\"A: nonDRNA\"] + cm_df[\"P: RNA\"][\"A: nonDRNA\"] \\\n",
        "        + cm_df[\"P: nonDRNA\"][\"A: DRNA\"] + cm_df[\"P: nonDRNA\"][\"A: RNA\"] + cm_df[\"P: DRNA\"][\"A: DRNA\"] \\\n",
        "      + cm_df[\"P: DRNA\"][\"A: RNA\"] + cm_df[\"P: RNA\"][\"A: DRNA\"] + cm_df[\"P: RNA\"][\"A: RNA\"] \n",
        "  DNA_FP = cm_df[\"P: DNA\"][\"A: nonDRNA\"] + cm_df[\"P: DNA\"][\"A: DRNA\"] + cm_df[\"P: DNA\"][\"A: RNA\"] \n",
        "  DNA_FN = cm_df[\"P: nonDRNA\"][\"A: DNA\"] + cm_df[\"P: DRNA\"][\"A: DNA\"] + cm_df[\"P: RNA\"][\"A: DNA\"]\n",
        "  mcc += confusion_metrics(DNA_TP, DNA_TN, DNA_FP, DNA_FN, \"DNA\")\n",
        "\n",
        "  # RNA - Ahmad \n",
        "  RNA_TP = cm_df[\"P: RNA\"][\"A: RNA\"]\n",
        "  RNA_TN = cm_df[\"P: nonDRNA\"][\"A: nonDRNA\"] + cm_df[\"P: DRNA\"][\"A: nonDRNA\"] + cm_df[\"P: DNA\"][\"A: nonDRNA\"] \\\n",
        "    + cm_df[\"P: nonDRNA\"][\"A: DRNA\"] + cm_df[\"P: nonDRNA\"][\"A: DNA\"] + cm_df[\"P: DRNA\"][\"A: DRNA\"] \\\n",
        "    + cm_df[\"P: DRNA\"][\"A: DNA\"] + cm_df[\"P: DNA\"][\"A: DRNA\"] + cm_df[\"P: DNA\"][\"A: DNA\"] \n",
        "  RNA_FP = cm_df[\"P: RNA\"][\"A: nonDRNA\"] + cm_df[\"P: RNA\"][\"A: DRNA\"] + cm_df[\"P: RNA\"][\"A: DNA\"] \n",
        "  RNA_FN = cm_df[\"P: nonDRNA\"][\"A: RNA\"] + cm_df[\"P: DRNA\"][\"A: RNA\"] + cm_df[\"P: DNA\"][\"A: RNA\"]\n",
        "  mcc += confusion_metrics(RNA_TP, RNA_TN, RNA_FP, RNA_FN, \"RNA\")\n",
        "\n",
        "  # DRNA - Ahmad \n",
        "  DRNA_TP = cm_df[\"P: DRNA\"][\"A: DRNA\"]\n",
        "  DRNA_TN = cm_df[\"P: nonDRNA\"][\"A: nonDRNA\"] + cm_df[\"P: RNA\"][\"A: nonDRNA\"] + cm_df[\"P: nonDRNA\"][\"A: RNA\"] \\\n",
        "    + cm_df[\"P: RNA\"][\"A: RNA\"] + cm_df[\"P: nonDRNA\"][\"A: DNA\"] + cm_df[\"P: RNA\"][\"A: DNA\"] \\\n",
        "    + cm_df[\"P: DNA\"][\"A: nonDRNA\"] + cm_df[\"P: DNA\"][\"A: RNA\"] + cm_df[\"P: DNA\"][\"A: DNA\"]\n",
        "  DRNA_FP = cm_df[\"P: DRNA\"][\"A: nonDRNA\"] + cm_df[\"P: DRNA\"][\"A: RNA\"] + cm_df[\"P: DRNA\"][\"A: DNA\"]\n",
        "  DRNA_FN = cm_df[\"P: nonDRNA\"][\"A: DRNA\"] + cm_df[\"P: RNA\"][\"A: DRNA\"] + cm_df[\"P: DNA\"][\"A: DRNA\"]\n",
        "  mcc += confusion_metrics(DRNA_TP, DRNA_TN, DRNA_FP, DRNA_FN, \"DRNA\")\n",
        "  \n",
        "  #nonDRNA - Tara\n",
        "  nonDRNA_TP = cm_df[\"P: nonDRNA\"][\"A: nonDRNA\"]\n",
        "  nonDRNA_TN = cm_df[\"P: RNA\"][\"A: RNA\"] + cm_df[\"P: RNA\"][\"A: DRNA\"] + cm_df[\"P: RNA\"][\"A: DNA\"] \\\n",
        "    + cm_df[\"P: DRNA\"][\"A: DRNA\"] + cm_df[\"P: DRNA\"][\"A: RNA\"] + cm_df[\"P: DRNA\"][\"A: DNA\"]  \\\n",
        "    + cm_df[\"P: DNA\"][\"A: DNA\"] + cm_df[\"P: DNA\"][\"A: DRNA\"] + cm_df[\"P: DNA\"][\"A: RNA\"]\n",
        "  nonDRNA_FN = cm_df[\"P: RNA\"][\"A: nonDRNA\"] + cm_df[\"P: DNA\"][\"A: nonDRNA\"] + cm_df[\"P: DRNA\"][\"A: nonDRNA\"]\n",
        "  nonDRNA_FP = cm_df[\"P: nonDRNA\"][\"A: DNA\"] + cm_df[\"P: nonDRNA\"][\"A: RNA\"] + cm_df[\"P: nonDRNA\"][\"A: DRNA\"]\n",
        "  mcc += confusion_metrics(nonDRNA_TP, nonDRNA_TN, nonDRNA_FP, nonDRNA_FN, \"nonDRNA\")\n",
        "\n",
        "  average_mcc = round(mcc/4,3)\n",
        "  accuracy4labels = round(100*(DNA_TP + RNA_TP + DRNA_TP + nonDRNA_TP)/ total_num_proteins,1)\n",
        "\n",
        "  print(\"\\nAverage MCC: \", average_mcc)\n",
        "  print(\"\\nAccuracy4Labels: \", accuracy4labels)\n",
        "\n",
        "perf_metrics(flattened_array_pred, flattened_array_exp)"
      ],
      "metadata": {
        "id": "jkB_bDtBsBTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0386a988-eb42-44d4-9e77-e56e808df203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            P: RNA  P: nonDRNA  P: DNA  P: DRNA\n",
            "A: RNA         164         359       0        0\n",
            "A: nonDRNA      25        7830       4        0\n",
            "A: DNA          12         362      16        1\n",
            "A: DRNA          0          20       0        2 \n",
            "\n",
            "DNA --------------------------------------------------\n",
            "Sensitivity: 4.1\n",
            "Specificity: 100.0\n",
            "Accuracy: 95.7\n",
            "MCC: 0.175 \n",
            "\n",
            "RNA --------------------------------------------------\n",
            "Sensitivity: 31.4\n",
            "Specificity: 99.6\n",
            "Accuracy: 95.5\n",
            "MCC: 0.489 \n",
            "\n",
            "DRNA --------------------------------------------------\n",
            "Sensitivity: 9.1\n",
            "Specificity: 100.0\n",
            "Accuracy: 99.8\n",
            "MCC: 0.246 \n",
            "\n",
            "nonDRNA --------------------------------------------------\n",
            "Sensitivity: 99.6\n",
            "Specificity: 20.8\n",
            "Accuracy: 91.2\n",
            "MCC: 0.401 \n",
            "\n",
            "\n",
            "Average MCC:  0.328\n",
            "\n",
            "Accuracy4Labels:  91.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Set"
      ],
      "metadata": {
        "id": "YXSasXjJNkAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/rams/fall 2022/CMSC 435/CMSC 435 Project\"\n",
        "df_test = create_pfeature(\"fake_test.txt\", \"protein_test.seq\", \"comp_test.txt\", 1)\n",
        "df_test = create_count_features(df_test)\n",
        "df_test = create_encoded_genome_col(df_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_uSbL0kNoz1",
        "outputId": "b15bc949-9dad-425f-a7a2-3e612e8fdef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1fuA8qot-MwgQ6H4rA5nuxA3l8Za7jqqS/rams/fall 2022/CMSC 435/CMSC 435 Project\n",
            "/content/drive/.shortcut-targets-by-id/1fuA8qot-MwgQ6H4rA5nuxA3l8Za7jqqS/rams/fall 2022/CMSC 435/CMSC 435 Project/pfeature_standalone\n",
            "INPUT DF SIZE:  (8795, 1)\n",
            "COMP DF SIZE (8795, 343)\n",
            "FINAL DF SIZE (8795, 344)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create n gram features -------------------------------------------------------------------\n",
        "df_test = df_test.astype(object)\n",
        "df_test.assign(two_gram_features = np.zeros([len(df_test),400]))\n",
        "\n",
        "for i, row in enumerate(df_test.sequence.to_numpy()):\n",
        "  whole_pair_list = two_or_three_gram(row,2) \n",
        "  divide_pair_list_into_groups = np.array(whole_pair_list).reshape(20,20)\n",
        "  list_of_counts = []\n",
        "  for group in divide_pair_list_into_groups:\n",
        "    # trying to find better ways to group these\n",
        "    # 1 way: take the sum of one group and make it that value, but scale it to be between 0 and 1\n",
        "    sum = np.sum(group) \n",
        "    sum = round(sum * .01, 2)\n",
        "    list_of_counts.append(sum) \n",
        "\n",
        "  # 2nd way: take the max value of that group, and add the index to it (index of the max has importance to it, adding is one way to show it)\n",
        "  #   group = np.array(group)\n",
        "  #   max = np.max(group)\n",
        "  #   index = group.argmax() \n",
        "  #   max_plus_index = round(max * .01, 2) + index\n",
        "  #   list_of_counts.append(max_plus_index) \n",
        "  df_test.at[i,\"two_gram_features\"] = list_of_counts\n",
        "\n",
        "print(\"DF TEST SHAPE \", df_test.shape)\n",
        "print(\"DF TEST COLUMNS\", df_test.columns)"
      ],
      "metadata": {
        "id": "AazybwsoVHRn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9302a3c-9930-4030-9b4d-99f317a0ef2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DF TEST SHAPE  (8795, 350)\n",
            "DF TEST COLUMNS Index(['sequence', 'CTC_111', 'CTC_112', 'CTC_113', 'CTC_114', 'CTC_115',\n",
            "       'CTC_116', 'CTC_117', 'CTC_121', 'CTC_122',\n",
            "       ...\n",
            "       'CTC_774', 'CTC_775', 'CTC_776', 'CTC_777', 'length', 'unique_val',\n",
            "       'unique_val_count', 'aa_count_freq', 'encoded', 'two_gram_features'],\n",
            "      dtype='object', length=350)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run model\n",
        "X_test, y_test, sm_test = prep_for_model(df_test, 1)\n",
        "X_test = selector.transform(X_test)\n",
        "curr_y_prediction = classifier.predict(X_test) \n",
        "df_test['label'] = curr_y_prediction #FIX THAT: \n",
        "\n",
        "%cd \"/content/drive/MyDrive/rams/fall 2022/CMSC 435/CMSC 435 Project\"\n",
        "df_test.to_csv(\"test_output.csv\")"
      ],
      "metadata": {
        "id": "LWIQBqsCDhRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df7af1fb-f5f4-4ae0-c204-4f28fbe0e7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL COLUMNS PRE Index(['sequence', 'CTC_111', 'CTC_112', 'CTC_113', 'CTC_114', 'CTC_115',\n",
            "       'CTC_116', 'CTC_117', 'CTC_121', 'CTC_122',\n",
            "       ...\n",
            "       'CTC_774', 'CTC_775', 'CTC_776', 'CTC_777', 'length', 'unique_val',\n",
            "       'unique_val_count', 'aa_count_freq', 'encoded', 'two_gram_features'],\n",
            "      dtype='object', length=350)\n",
            "FINAL COLUMNS POST Index(['CTC_111', 'CTC_112', 'CTC_113', 'CTC_114', 'CTC_115', 'CTC_116',\n",
            "       'CTC_117', 'CTC_121', 'CTC_122', 'CTC_123',\n",
            "       ...\n",
            "       'CTC_771', 'CTC_772', 'CTC_773', 'CTC_774', 'CTC_775', 'CTC_776',\n",
            "       'CTC_777', 'length', 'aa_count_freq', 'two_gram_features'],\n",
            "      dtype='object', length=346)\n",
            "AA COLUMNS 20\n",
            "FEATURE VECTOR LENGTH:  384\n",
            "['sequence_length', 'aa_0', 'aa_1', 'aa_2', 'aa_3', 'aa_4', 'aa_5', 'aa_6', 'aa_7', 'aa_8', 'aa_9', 'aa_10', 'aa_11', 'aa_12', 'aa_13', 'aa_14', 'aa_15', 'aa_16', 'aa_17', 'aa_18', 'aa_19', 'aa_20', 'aa_21', 'aa_22', 'aa_23', 'aa_24', 'aa_25', 'aa_26', 'aa_27', 'aa_28', 'aa_29', 'aa_30', 'aa_31', 'aa_32', 'aa_33', 'aa_34', 'aa_35', 'aa_36', 'aa_37', 'aa_38', 'aa_39', 'aa_40', 'aa_41', 'aa_42', 'aa_43', 'aa_44', 'aa_45', 'aa_46', 'aa_47', 'aa_48', 'aa_49', 'aa_50', 'aa_51', 'aa_52', 'aa_53', 'aa_54', 'aa_55', 'aa_56', 'aa_57', 'aa_58', 'aa_59', 'aa_60', 'aa_61', 'aa_62', 'aa_63', 'aa_64', 'aa_65', 'aa_66', 'aa_67', 'aa_68', 'aa_69', 'aa_70', 'aa_71', 'aa_72', 'aa_73', 'aa_74', 'aa_75', 'aa_76', 'aa_77', 'aa_78', 'aa_79', 'aa_80', 'aa_81', 'aa_82', 'aa_83', 'aa_84', 'aa_85', 'aa_86', 'aa_87', 'aa_88', 'aa_89', 'aa_90', 'aa_91', 'aa_92', 'aa_93', 'aa_94', 'aa_95', 'aa_96', 'aa_97', 'aa_98', 'aa_99', 'aa_100', 'aa_101', 'aa_102', 'aa_103', 'aa_104', 'aa_105', 'aa_106', 'aa_107', 'aa_108', 'aa_109', 'aa_110', 'aa_111', 'aa_112', 'aa_113', 'aa_114', 'aa_115', 'aa_116', 'aa_117', 'aa_118', 'aa_119', 'aa_120', 'aa_121', 'aa_122', 'aa_123', 'aa_124', 'aa_125', 'aa_126', 'aa_127', 'aa_128', 'aa_129', 'aa_130', 'aa_131', 'aa_132', 'aa_133', 'aa_134', 'aa_135', 'aa_136', 'aa_137', 'aa_138', 'aa_139', 'aa_140', 'aa_141', 'aa_142', 'aa_143', 'aa_144', 'aa_145', 'aa_146', 'aa_147', 'aa_148', 'aa_149', 'aa_150', 'aa_151', 'aa_152', 'aa_153', 'aa_154', 'aa_155', 'aa_156', 'aa_157', 'aa_158', 'aa_159', 'aa_160', 'aa_161', 'aa_162', 'aa_163', 'aa_164', 'aa_165', 'aa_166', 'aa_167', 'aa_168', 'aa_169', 'aa_170', 'aa_171', 'aa_172', 'aa_173', 'aa_174', 'aa_175', 'aa_176', 'aa_177', 'aa_178', 'aa_179', 'aa_180', 'aa_181', 'aa_182', 'aa_183', 'aa_184', 'aa_185', 'aa_186', 'aa_187', 'aa_188', 'aa_189', 'aa_190', 'aa_191', 'aa_192', 'aa_193', 'aa_194', 'aa_195', 'aa_196', 'aa_197', 'aa_198', 'aa_199', 'aa_200', 'aa_201', 'aa_202', 'aa_203', 'aa_204', 'aa_205', 'aa_206', 'aa_207', 'aa_208', 'aa_209', 'aa_210', 'aa_211', 'aa_212', 'aa_213', 'aa_214', 'aa_215', 'aa_216', 'aa_217', 'aa_218', 'aa_219', 'aa_220', 'aa_221', 'aa_222', 'aa_223', 'aa_224', 'aa_225', 'aa_226', 'aa_227', 'aa_228', 'aa_229', 'aa_230', 'aa_231', 'aa_232', 'aa_233', 'aa_234', 'aa_235', 'aa_236', 'aa_237', 'aa_238', 'aa_239', 'aa_240', 'aa_241', 'aa_242', 'aa_243', 'aa_244', 'aa_245', 'aa_246', 'aa_247', 'aa_248', 'aa_249', 'aa_250', 'aa_251', 'aa_252', 'aa_253', 'aa_254', 'aa_255', 'aa_256', 'aa_257', 'aa_258', 'aa_259', 'aa_260', 'aa_261', 'aa_262', 'aa_263', 'aa_264', 'aa_265', 'aa_266', 'aa_267', 'aa_268', 'aa_269', 'aa_270', 'aa_271', 'aa_272', 'aa_273', 'aa_274', 'aa_275', 'aa_276', 'aa_277', 'aa_278', 'aa_279', 'aa_280', 'aa_281', 'aa_282', 'aa_283', 'aa_284', 'aa_285', 'aa_286', 'aa_287', 'aa_288', 'aa_289', 'aa_290', 'aa_291', 'aa_292', 'aa_293', 'aa_294', 'aa_295', 'aa_296', 'aa_297', 'aa_298', 'aa_299', 'aa_300', 'aa_301', 'aa_302', 'aa_303', 'aa_304', 'aa_305', 'aa_306', 'aa_307', 'aa_308', 'aa_309', 'aa_310', 'aa_311', 'aa_312', 'aa_313', 'aa_314', 'aa_315', 'aa_316', 'aa_317', 'aa_318', 'aa_319', 'aa_320', 'aa_321', 'aa_322', 'aa_323', 'aa_324', 'aa_325', 'aa_326', 'aa_327', 'aa_328', 'aa_329', 'aa_330', 'aa_331', 'aa_332', 'aa_333', 'aa_334', 'aa_335', 'aa_336', 'aa_337', 'aa_338', 'aa_339', 'aa_340', 'aa_341', 'aa_342', 'aa_343', 'aa_344', 'aa_345', 'aa_346', 'aa_347', 'aa_348', 'aa_349', 'aa_350', 'aa_351', 'aa_352', 'aa_353', 'aa_354', 'aa_355', 'aa_356', 'aa_357', 'aa_358', 'aa_359', 'aa_360', 'aa_361', 'aa_362', 'aa_363', 'aa_364', 'aa_365', 'aa_366', 'aa_367', 'aa_368', 'aa_369', 'aa_370', 'aa_371', 'aa_372', 'aa_373', 'aa_374', 'aa_375', 'aa_376', 'aa_377', 'aa_378', 'aa_379', 'aa_380', 'aa_381', 'aa_382']\n",
            "/content/drive/.shortcut-targets-by-id/1fuA8qot-MwgQ6H4rA5nuxA3l8Za7jqqS/rams/fall 2022/CMSC 435/CMSC 435 Project\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "252a96aad647c7060075ad65005bf569998017e265935679af6b44aa4620e047"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}